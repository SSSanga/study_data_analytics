{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3991cd60-afc4-4529-990d-789c47cc3ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"오늘 날씨가 좋아서 나들이 가고 싶다.\", # -> [오늘, 날씨가, 좋아서 ...]\n",
    "    \"이 영화는 정말 재미있었어요.\",\n",
    "    \"맛있는 음식을 먹으러 갈까요?\",\n",
    "    \"운동을 하면 건강에 좋아지는 것 같아요.\",\n",
    "    \"공부하기 싫어서 미루고 있어요.\",\n",
    "    \"여행 계획을 세우고 있는데 어디로 갈까요?\",\n",
    "    \"좋은 책을 읽으면 마음이 편안해져요.\",\n",
    "    \"오늘은 친구들과 만나서 재미있게 놀았어요.\",\n",
    "    \"새로운 언어를 배우는 것은 어려워도 흥미로워요.\",\n",
    "    \"주말에 가족들과 함께 시간을 보내기로 했습니다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f360e28-fc31-49c4-a577-b6d424b2e8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mecab import MeCab\n",
    "mecab=MeCab()\n",
    "# 불용어 리스트 생성 (예시)\n",
    "stopwords = ['가', '고', '을', '를', '이', '는']\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# Okt 형태소 분석기 인스턴스 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 불용어 리스트 생성 (예시)\n",
    "stopwords = ['가', '고', '을', '를', '이', '는']\n",
    "\n",
    "# 토크나이징 함수 정의\n",
    "def tokenizer(raw, pos=[\"Noun\",\"Alpha\",\"Verb\",\"Number\"], stopword=stopwords):\n",
    "    return [\n",
    "        word for word, tag in okt.pos(\n",
    "            raw, \n",
    "            norm=True,   # normalize 그랰ㅋㅏ -> 그래ㅋㅋ\n",
    "            stem=True    # stemming 바뀌나->바뀌다\n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopword\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ab2efa-fc6f-408d-84e1-b19ae8d91d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '날씨', '나들이', '가다', '싶다']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad1fec-a1a3-4d42-9c5b-fb9432af826b",
   "metadata": {},
   "source": [
    "### tokenizer 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b395e-bee3-4f1c-a4ef-3f5611033545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징 함수 정의, 형태소 분석기를 바꾸고 싶으면 Okt만 바꾼다. \n",
    "def tokenizer_decomposition(raw, pos=[\"Noun\",\"Alpha\",\"Verb\",\"Number\"], stopword=stopwords):\n",
    "    return [\n",
    "        word for word, tag in okt.pos(\n",
    "            raw, \n",
    "            norm=True,   # normalize 그랰ㅋㅏ -> 그래ㅋㅋ\n",
    "            stem=True    # stemming 바뀌나->바뀌다\n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopword\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60428190-5687-4b80-b788-6c85bdfe79f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'Noun'),\n",
       " ('날씨', 'Noun'),\n",
       " ('가', 'Josa'),\n",
       " ('좋다', 'Adjective'),\n",
       " ('나들이', 'Noun'),\n",
       " ('가다', 'Verb'),\n",
       " ('싶다', 'Verb'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.pos(sentences[0],norm=True,stem=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "413dd410-808c-4472-993b-38738d7ce7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw = sentences[0]\n",
    "pos = [\"Noun\",\"Alpha\",\"Verb\",\"Number\"]\n",
    "stopword = ['가', '고', '을', '를', '이', '는']\n",
    "result_list = list()\n",
    "for word, tag in okt.pos(raw,norm=True,stem=True) :\n",
    "    if len(word) > 1 and tag in pos and word not in stopword :\n",
    "        result_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6134d407-3691-4685-97b0-db18c3fc8253",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '날씨', '나들이', '가다', '싶다']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbde75a-c429-4d2b-8df3-84bd582244f0",
   "metadata": {},
   "source": [
    "이걸 다시 넣으면 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b1b2a69-e302-49f8-a466-07341623f9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenizer_decomposition(raw, pos=[\"Noun\",\"Alpha\",\"Verb\",\"Number\"], stopword=stopwords):\n",
    "    result_list = list()\n",
    "    for word, tag in okt.pos(raw,norm=True,stem=True) :\n",
    "        if len(word) > 1 and tag in pos and word not in stopword :\n",
    "            result_list.append(word)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34e9cf6a-ac26-4863-800f-907a52011dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '날씨', '나들이', '가다', '싶다']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_decomposition(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbd759-9779-4816-8c6b-293a60461b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
